---
title: "My Coursera Project"
author: "Adeniran Dotun Michael"
date: "2022-08-07"
output: html_document

---

# Introduction

This is a capstone Project for the Google Data Analytics Professional Certification. BellaBeat is a high-tech company that manufactures health-focused smart products that help women easily, their overall health and wellness, and get connected to their body and mind throughout different stages in life.


I will be using the 6 phases of the analysis process-Ask, Prepare, process, Analyse, Share, and Act-to 


help guide my analysis of the datasets.

## Phase 1: Ask 


### 1. Identify the business task:


  * Analyse smart devices usage data
  
  
  * Provide high quality recommendations for Bellabeat’s marketing strategy
  
  
 Questions to guide analysis:


  * What are some of the trends in smart device usage?
  
  
  * How can you apply these trends to Bellabeat customers?
  
  
  * How can these trends help influence Bellabeat marketing strategy?
  
  
### 2. Consider key stakeholders

Primary Stakeholder(s):


  * Urska Srsen - Chief Executive Officer (CEO) and co-founder of Bellabeat
  
  
  * Sando Mur - Mathematician and co-founder of Bellabeat
  
  
Secondary Stakeholder:


  * Bellabeat Marketing Analytics Team


### Phases 2 & 3: Prepare & Process 


#### 1. Identify the data source:

Dataset: FitBit Fitness Tracker Data (CC0: Public Domain, dataset made available by Mobius). Click here to access the dataset –> [link](https://www.kaggle.com/datasets/arashnic/fitbit): This Kaggle dataset contains a personal fitness tracker from thirty FitBit users. Thirty eligible FitBit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. It includes information about daily activity, steps, and heart rate that can be used to explore users’ behaviours and habits. These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between April 2016 to May 2016.



#### 2. Determine the credibility of the data:


I will use the “ROCCC” system to determine the credibility and integrity of the data:


**Reliability:** This data is not reliable. There is no information about the margin of error and a small sample size (30 participants) has been used, which can limit the amount of analysis that can be done.


**Originality:** This is not an original dataset as it was originally collected from Amazon Mechanical Murk.


**Comprehensiveness:** This data is not comprehensive. There is no information about the participants, such as gender, age, health state, etc. This could mean that data was not randomised. If the data is biased, then the insights from the analysis will be unfair to all types of people.


**Current:** This data was collected in 2016, which means it is currently outdated and may not represent the current rends in smart device usage.


**Cited:** As stated before, Amazon Mechanical Murk created the dataset, but we have no information on whether this is a credible source.


The data integrity and credibility is clearly insufficient to provide reliable and comprehensive insights to Bellabeat. Therefore, the following analysis can only provide first hints and directions which should be verified through an analysis of a larger and much more reliable dataset.


#### 3. Sort and filter the data


For this analysis, I will be focusing on the daily data as my analysis will be on detecting high-level trends in smart device usage. I will be using the ‘dailyActivity_merged’, and ‘sleepDay_merged’ datasets as they will probably give some interesting insights into the user data.


NOTE: All my analysis will be completed in RStudio Cloud.


I will start by installing and loading the required R packages for my analysis.

```{r}
library(tidyverse)
```
```{r}
library(dplyr)
library(readr)
library(magrittr)
```

```{r}
library(tidyr)
library(devtools)
```

```{r}
library(lubridate)
```
```{r}
library(janitor)
```
```{r}
library(ggplot2)
```

Next i will import the necessary files into R 

```{r}
daily_intensities <- read_csv("dailyIntensities_merged.csv")
sleep_day <- read_csv("sleepDay_merged.csv")
```
### CLEANING DATA SETS
##### let's have a close look at each dataset
```{r}
head(daily_intensities)
```

```{r}
head(sleep_day)
```

From the above i can see that the date and time are in the same column in the 'sleepDay' datasets.i will use separate function to separate them into two columns.

```{r}
sleep_day_new <- sleep_day %>% 
  separate(SleepDay, c('Date', "Time"), " ")
```
I will now identify all the columns in each dataframe
```{r}
colnames(daily_intensities)
```
```{r}
colnames(sleep_day_new)
```
I will now find how many distinct users there are in each dataframe
```{r}
n_distinct(daily_intensities$Id)
```
```{r}
n_distinct(sleep_day_new$Id)
```
 From this, we can see that there are:
* 33 unique participants in the daily_intensities dataframe
* 24 unique participants in the sleep_day_new dataframe

I will now check to see how many observations there are in each dataframe
```{r}
nrow(daily_intensities)
```
```{r}
nrow(sleep_day_new)
```
From this we can see that there are: 940 rows in the daily_intensities dataframe
From this we can see that there are: 413 rows in the sleep_day_new dataframe

Next i will check to see if there are any duplicates rows in each data frame
```{r}
nrow(daily_intensities[duplicated(daily_intensities),])
```
```{r}
nrow(sleep_day_new[duplicated(sleep_day_new),])
```
 We can now see that three duplicates were found in the sleep_day_new dataframe and will have to be removed
```{r}
nrow(sleep_day_new)
```
```{r}
sleep_day_new <- unique(sleep_day_new)
```
```{r}
nrow(sleep_day_new)
```
While exploring the datasets, I also found a lot of cells with "0"values, so i will omit these to prevent skewed results
```{r}
daily_intensities <- daily_intensities %>% filter(SedentaryActiveDistance !=0)
daily_intensities <- daily_intensities %>% filter(ModeratelyActiveDistance!=0)
```
# PHASE 4: ANALYZE
I will now check for the statistical summary of the variables in each dataframe

#### 1.daily_intensities dataframe
```{r}
daily_intensities %>% 
  select(LightlyActiveMinutes,
         FairlyActiveMinutes,
         VeryActiveMinutes,
         SedentaryActiveDistance,
         LightActiveDistance,
         ModeratelyActiveDistance,
         VeryActiveDistance) %>% 
  summary()
```
Observations:


  * Average sedentary active minutes was 0.02000 minute  


  * Average very active minutes and fairly active members minutes were at 1.105 minute and 15.20 minutes  respetively


  * Average lightly active minutes were at 4.382 minutes  


  * The average amount of moderately active minutes were at 0.7185 minute

Deductions:  
  
  
  * Participants were largely inactive throughout the day  
  
  
  * Participants spent a low amount of time exercising  
  
  
  * Participants are unlikely to take part in vigorous activities

#### 2. Sleep_day_new dataframe

```{r}
sleep_day_new %>% 
  select(TotalSleepRecords,
         TotalMinutesAsleep,
         TotalTimeInBed) %>% 
  summary()
```
Observations:
* Participants spent,on average 458.6 minutes (7.64 hours) in bed
* Average sleeping time was 419.2 minutes or 7 hours
* Participants slept once per day on average

Deductions:
* Participants had an adequate amount of sleep

# PHASE 5: Share Visualizations
#### I will create visualizations to find relationships between the variables.

We will first look at the relationship between the total number of active time and amount of sleep
```{r}
ggplot(data = daily_intensities) + 
  geom_point(mapping = aes(x=VeryActiveMinutes, y=SedentaryActiveDistance), color="red") + 
  geom_smooth(mapping = aes(x=VeryActiveDistance, y=SedentaryActiveDistance)) + 
  labs(title = "The relationship between Very active minutes and sedentary active distance", x="Very Active Minutes", y="sedentary active distance")
```

This graph shows a negative correlation between very active minutes and sedentary distance

Let's check the result between Very Active minutes and lightly active members
```{r}
ggplot(data = daily_intensities) + 
  geom_point(mapping = aes(x=VeryActiveMinutes, y=LightlyActiveMinutes), color="pink") + 
  geom_smooth(mapping = aes(x=VeryActiveMinutes, y=LightlyActiveMinutes)) + 
  labs(title = "The relationship between Very active distance and Lightly active minutes", x="sedentary active distance", y="Very Active Minutes")
```


Let's check the relationship between Very Active minutes and very active distance


```{r}
ggplot(data = daily_intensities) + 
  geom_point(mapping = aes(x=VeryActiveMinutes, y=VeryActiveDistance), color="purple") + 
  geom_smooth(mapping = aes(x=VeryActiveMinutes, y=VeryActiveDistance)) + 
  labs(title = "The relationship between Very active minutes and very active distance", x="very active minutes", y="Very Active distance")
```


Observation: As the distance increases, the very active number increases too.


Let's check the relationship between Moderately active distance and fairly active minutes


```{r}
ggplot(data = daily_intensities) + 
  geom_point(mapping = aes(x=FairlyActiveMinutes, y=ModeratelyActiveDistance), color="gold") + 
  geom_smooth(mapping = aes(x=FairlyActiveMinutes, y=ModeratelyActiveDistance)) + 
  labs(title = "The relationship between Very active minutes and farily active distance", x="Fairly active minutes", y="moderately Active distance")
```


Observation: As the Moderately active distance increases the fairly active minute increases too.


Let's plot total minutes asleep against total time in bed


```{r}
ggplot(data = sleep_day_new,mapping = aes(x=TotalMinutesAsleep, y=TotalTimeInBed, color="orange")) + 
  geom_point() + 
  geom_smooth() + 
  labs(titel= "The relationship between the Total Minutes Asleep and the Total time in Bed", x="Total minutes asleep", y="Total minutes in bed")
```
As you can see from the graph above, there is a positive correlation between the total minutes asleep and the amount of time spent in bed. Using this data, Bellabeat can use an app that notifies its customers about when it would be the right time to go to bed so that they can get an adequate amount of sleep.

# Phase 6: Act

Recommendations for Bellabeat Marketing Strategy:
* Based on the activity levels and amount of sleep day, users appear to burn more active when the activity isn't too stressful. Therefore, Bellabeat should encourage users to exercise more through reminders. They could also offer app incentives, such as give users app credits for every 1000 steps, which can then be used to redeem prizes or vouchers.

* The data also shows many people lead either a lightly active or sedentary lifestyle, which may be due to the nature of their work or the lack of time to exercise. Bellabeat could have a section on their app for short workout videos or short exercises (for example, 10 minute videos) that their customers can follow along to if they don’t necessarily want to exercise alone.

* To encourage better sleeping habits, Bellabeat could incorporate reminders through an app that notifies users of the best time to go to sleep and wake up in order to feel refreshed in the morning and get adequate amount of sleep. The app could also automatically turn on ‘do not disturb’ mode and turn on ‘night mode’ on the customers’ phones to signal the user that they are not disturbed by messages or phone calls from family and friends.


Recommendations based on the limitations of the dataset:
* A larger sample size in order to improve the statistical significance of the analysis.

* Collect a longer period of tracking data, ideally for 6 months to a year, to account for behavioural changes due to the changes in seasons.

* The need to obtain current data in order to better reflect current consumer behaviours and/or trends in smart device usage.

* Collect data from internal sources (if possible) and/or from primary/secondary data sources to increase credibility and reliability of the datasets.
